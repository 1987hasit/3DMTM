%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bin Liang (bin.liang.ty@gmail.com)
% Charles Sturt University
% Created:	September 2013
% Modified:	November 2013
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Gesture recognition Version 1.0 19/11/2013

Dataset: MSR Action3D dataset
This code is performed well on MSR Actino3D dataset and a paper has been 
accepted by ICCV workshop 2013.

HOW to use:

(1) Two SVM files can be generated after running the program, namely "TR_Gestures.svm"
    and "TE_Gestures.svm";
(2) "TR_Gestures.svm" and "TE_Gestures.svm" are used as inputs for LIBSVM. The commands are:
    a.  ## data scale
        svm-scale.exe -l -1 -u 1 -s t_range TR_Gestures.svm > TR_Gestures.svm.scale 
        svm-scale.exe -r t_range TE_Gestures.svm > TE_Gestures.svm.scale
    b.  ## find out the best C and gamma for the SVM model
        python.exe grid.py TR_Gestures.svm.scale
    b.  svm-train.exe -c [best C] -g [] TR_Gestures.svm.scale
        svm-predict.exe TE_Gestures.svm.scale TR_Gestures.svm.scale.model TE_Gestures.svm.predict